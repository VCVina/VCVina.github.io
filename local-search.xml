<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>日本和中国的大学教育</title>
    <link href="/2020/05/05/twoCountryCollege/"/>
    <url>/2020/05/05/twoCountryCollege/</url>
    
    <content type="html"><![CDATA[<p>原文首次发布于2018年，于今日经过新的补充后重新上线。<br><a id="more"></a></p><blockquote><p>人生的一切修业，有含蓄于内与发表于外两种，要两者兼修。<br>今日的学者，大多只埋首于自己知识的充实，却缺乏自觉把知识传递给人的责任。这事有必要好好地反省。自己充实知识，深入思考，就必须如探无底深渊；但是，与人接触活用知识，就必须如鸟在天空一般自在飞翔。学识的致密，向内则无穷，其活用的广大，向外则无限，这样始足以称为真正的学者。<br>——福泽谕吉《劝学》</p></blockquote><h1 id="学业"><a href="#学业" class="headerlink" title="学业"></a>学业</h1><p>2018年的时候，我去了日本读了几天书。并非长期留学亦或是短期旅游，只有少少半个月，说长不长，说短也不短，长却不至于忘记中国生活的样子，短也不至于记不住山手线该怎么走。这趟来日本让我感到颇为自豪：因为我脚下接触的这个国家，似乎和我脑中所构想的国家十分相似。</p><p>这只是国家的文化画像罢了。作为一个从小接触亚洲文化长大的人，韩国和日本一直都是我最好奇也是最希望亲自去逛一逛的国家。曾经心目中对于日本国的教育画像有着虚无缥缈的期待，但是随着事件的进步，自己的想法也慢慢地成熟了起来。</p><p>因此我希望靠着这次难得的经历去日本的大学中探寻一番。这次我去日本的半个月前往的是某所大学，我暂且命名为R，而我在的院校暂且命名为N。因为两校之间「理论上」不应该会有大的差距，因此两校之间的水平让我这种咸鱼来进行鉴赏真是再合适不过了。</p><p>那么结论是什么呢？</p><p>我姑且夸大着说：如果能把N的学生和R的学生进行交换，那么R可以在教育环境、教学水准方面全线击败N。</p><p>其实这个结论是很明显的：如果教育界能将一个学校划分为三个模块：学校作为环境提供的资源En、被选拔出的学生展现的学习能力Le、教授讲师所展现的教学能力Te，那么三模块之间两校的差距应该是：</p><blockquote><p>R.En &gt;&gt; N.En<br><br/> R.Le &lt; N.Le<br><br/>R.Te &gt; N.Te</p></blockquote><p>为什么会这么说？为了给自己一点自信心，我先说说N能击败R的一点，就是学生。</p><p>我们非常清楚日本的就业率一直都是处于令人羡慕的高度（就算算入日韩贸易战期间双方的经济冰河，其全国失业率也只有4.6%），因此大概是因为这个，导致日本的学生平均来看不如中国学生热爱学习——贬义和褒义并存。</p><p>R校混日子的学生远比我想想的要多，而且这个现象似乎在学长那边已经得到了足以说服我的解释：「既然我和顶尖学校毕业的人工资能差不多，那我还努力干什么」。</p><p>说的太对了，也一时让我激起了一丝「此生若此便是安好」的羡慕想法。</p><p>日本的年功序列似乎是带给了各层次学生都相对富足的毕业前途，而它的出现也确实大幅度压低了贫富差距：白领和蓝领的收入就是一纸之隔罢了。</p><p>不过宽松的环境似乎成了学生们的温室，而这个美丽温室中的花朵是否只要慢慢地度过四年大学时光，然后等待毕业毕业就好了呢。在中国的高度竞争压力下，光是毕业根本无法满足这个国家年轻人日益膨胀的物质需求，而相对应的「每干1年月薪加500元」的年功序列究竟是好还是不好呢？这就因人而异了吧，虽然说看上去有一点学习无用论的意思，但是一个允许人偷懒的国家，是不是也算是一个很棒的国家呢？</p><p>R校作为一所著名学校，刚开学的时候，情报工学的学生就要上计算机课群中最重要专业课之一：Operation System。虽然是最重要的课程之一，但是在讲师开始上课以后十分钟内都还有学生才在这时陆续进场。而随机抽查的环节，有关事先就已发放的オペレーティングシステン讲义也是有不少人一问三不知，在教材上来看，割込み在CPU中的作用这种看过书就知道的内容随机抽了一个人也回答不上来，虽然主动发言的同学也有不少（而且全都是正确的），但是在我感受看来，混日子的学生看上去比中国的学校要多的多。</p><p><img src="/img/postsImg/twoCountryCollege/os1.jpeg" srcset="/img/loading.gif" alt="image"></p><p>但是我们一转话题到学校上，那N可就只能望洋兴叹了：R有着比N不知道优秀多少的硬件设施，暂且不说无限续杯的学校咖啡厅、带榻榻米的便利店和数都数不清的自习区域，单说实验室就已经令人感受到了两者之间的差距。</p><p>因为专业问题，我前往的实验室是有关图形研究的实验室。R作为一个在日本算不上最顶尖的学校，竟然有着单个实验室上千万的不可思议的科研经费容量，真是令我感到费解。该实验室甚至出现了一个奇怪的现象，就是「钱花不完就要赶紧花」，而且还不止这家实验室，其他的实验室也是如此。根据内部人员的说法，大家这么做的原因是因为防止自己下次的预算被紧缩：「你不花完钱就表示你不需要，所以下次就不给你这么多钱，因此你要赶紧花」，然后给每个人配置了最高配的电脑。</p><p>当我看到他们能做自己的实验，有自己的设备，还能因为日本和美国关系好搞到一些来自美国的未公开新型实验设备，或者能和世界各国高级学者随时进行交流的时候，一种奇怪的酸味油然而生。</p><p><img src="/img/postsImg/twoCountryCollege/unitychan.jpeg" srcset="/img/loading.gif" alt="image"></p><p>相比之下我校地理位置延展的教育资源贫瘠，拨款限制极大，要一台设备需要上下审批一个月，而R校只需要和教授说一声简单审批两关就可以，在检查滥用情况和公款私用情况方面R放的更开，更加信任自己的研究人员是一个不乱花钱的好人；而我校采用的是「你极有可能公款私用」的有罪推定思想，自然很难得到财力去干一些耗资巨大的实验，然后就只能接企业单子、上几门课、帮企业打工、接企业单子、上几门课…以此类推。</p><p>因为本人只是去R的实验室停留了极少的时间，因此本部分只能根据两位学长对我说的印象进行综述——研究自由，经费充足。这两方面完全是因学校或研究室或教授不同而不同，因此不能笼统的套在全日本的高等教育上。根据学长的说法，自己想要什么课题，只要和教授提出他就会帮你选出好的课题，或者说直接按照你说法作为课题来鼓励你继续（这是实验室B），而实验室A是一个华裔教授，实验室中给我们的带队学长是一个获得文部省奖学金的好学生，按照他的说法（下列为转述，并非原文）</p><p>「教授他是真的喜欢这个方面的东西，他真的喜欢研究。」</p><p>「教授平常很忙，总是一天到晚往国外跑参加各种学会，但是只要在学校里一天，他都会在实验室常驻，随时等候我们给他们问问题。这种会好好教学的教授我觉得真的很难得。」</p><p>上面是学长尤为强调的一点，当然他也说了：教授态度因人而异，不能套用在整个学界。但是至少在他这个实验室和隔壁实验室B的氛围并没有国内传言的那么封闭死板，隔壁B的教授甚至可以被学生传教参与到ACG的研究中来。</p><p>总结来说，R校给我的感觉就是</p><p>「我知道我们学校还不够好，但是我希望我能给你们提供我能提供的最好的东西，我希望你们能用这些成为一个有才能的人来回馈我。我希望我的本科生能有最好的学习环境，我希望我的修士博士可以飞往全世界参加尖端学会，交通费生活费我能给报销就给报销…」</p><p>而N校给我的感觉就是：</p><p>「我们学校曾经很好现在也很好，你敢说我不好我就敢让你退学。钱是我的，你想用必须和我说，不然我怕你拿去买零食，当然钱是大事，所以审批一个月你不能抱怨。你毕业以后一辈子都是我校人，一定要知恩图报，虽然我们现在连自动冲水的水费都给不起但是我们有钱挖路造公园，所以你也不能说我们穷。」</p><p>同时，N校的思维方式其实也正是典型的战狼思维：我是你爹，你不孝敬我孝敬谁？</p><p>如果中国的学校，能够更加相信自己的老师和学生，应该会有更大的进步。</p><p>虽然，我对这个学校的进步没有任何兴趣。</p><h1 id="学业后的一切"><a href="#学业后的一切" class="headerlink" title="学业后的一切"></a>学业后的一切</h1><p>上文我说到了日方中方的学生学业素质对比，以及硬件设施的一半，并得出了日方的R校在学生上不如我所在的N校，以及在学习环境和基础设备上N不如R的结论，下文我将讲述一些激进且跑题的话题。</p><p>在讲到学校环境之前，我有必要简单的表明一下我对于学费和环境的看法。目前有很多人认为中国的高校环境恶劣是由于学费过于低廉导致的，一般该看法的持有者们会有如下认知：</p><p>「中国学生既然享受到了低廉的学费，那么周围的硬件环境相对落后也是很正常的，不能用日方私立学校作为对比的条件。」</p><p>简直是富有中国特色流氓逻辑的胡扯，究竟是谁规定低廉的学费是有必要的？</p><blockquote><p>实业之不发达，厥有二因：一在教育之幼稚，一在资本之缺少。无论何项实业，皆与科学相关，理化之不知，汽电之不讲，人方以学战、以商战，我则墨守旧法，迷信空谈。余愿国民输入外国文明教育，即政治、法律等学亦皆有实际而无空言，余对于教育之观念如是。——袁世凯</p></blockquote><p>什么是教育之幼稚，什么是资本之缺少，袁世凯想必是不知道的，因为到现在中国人依然不知道究竟成熟的教育意味着什么，就像鲁迅没有解决的问题到现在中国人依然没有解决一样。</p><p>即低廉的学费是否真的有必要。如果提供了更高的学费就能获得更优秀的环境和教育，那么强制他们享受低廉的学费就不能被看作是一种享受，而是一种基于政治执行力的强制决策，这就意味着就算你有钱我也强迫你接受更加脏乱差的环境，哪怕你完全有能力负担更好的。</p><p>请问凭什么？</p><p>更高的学费并不是意味着更优秀的教育，例如清华大学的学费很显然低于许多私立独立学院，但是这并不代表清华的教育就落后于其他学校，可是如果我在对比的是同样水平的两所大学呢？</p><p>「当你在同样分数的情况下，你愿意交更多的钱享受到更好的教育吗」</p><p>有人看到这里会想到一个问题：「中国教育资源这么稀缺，如果允许高昂的学费那穷苦人家怎么办？」</p><p>这个问题问得太好了，可是这些教育资源的问题为何要询问负担得起学费的中产阶级呢？学校不是他们开办的，教育资源贫瘠也不是他们干的，他们没有行政权力为何要怪罪到他们身上？</p><p>换句话说，你究竟在害怕什么？</p><p>这从某种意义上说就是资本主义的体现了——我有钱，我为什么不能在同样的分数、同样的选择区间中选择更好的教育？或者单纯的获得更好的环境呢？一个中产家庭，如果可以负担得起一年三万的高等教育，一般来说就不会去选择一年六千的教育，因为对于作为长期投资的教育而言，现在多投入就意味着以后子女更好的前途，我为何要为了所谓的平等让自己子女遭受不必要的苦难呢？</p><p>当我无法决定我的出身之时，你为什么连我用自己赚来的钱换更好的人生的权利都要剥夺？中国的教育性价比真的足够高了吗？中国的教育真的麻雀虽小五脏俱全吗？</p><p><img src="/img/postsImg/twoCountryCollege/abe.jpeg" srcset="/img/loading.gif" alt="image"></p><p>为什么东京大学的学生们可以在学校里讨论安倍晋三的行政目的，而我们不能讨论呢？</p><p>为什么早稻田大学的学生们可以讨论日韩关系恶化和日本究竟哪儿走错了，而我们不能讨论呢？</p><p>中国的学生到底做错了什么？我们到底是为了什么？</p><blockquote><p>你究竟是忘了，还是害怕想起来？<br>——《返校》</p></blockquote><p>所谓日本被一颗核弹轰老实了，完全一派胡言。这个国家作为上世纪中国自然科学的导师，用二十年中获得的二十个诺贝尔奖告诉中国的爱国者们什么叫做绝对的压制。</p><p>今后的我，或者说我们应该好好的审视自己的成见了。</p><p>我曾经没有做错什么，却用一次充满后悔和失败的结果为了一场肺炎买单。我没有做错任何事，为什么会招到这样的结果？</p><p>所以其实我归根到底从一开始就做错了：曾经的我心中蕴藏的乐观主义，告诉我就算在国内也一切都可以改变。</p><p>现在的我知道自己做错了，不是所有的问题都是有答案的，不是所有的失望都伴随着希望。</p><p>所谓的乐观主义，本质上就是一张废纸罢了，我所追求的东西根本从一开始就不存在，就像林语堂所说：<strong>我就像一个孩子在玩弄盲人手中的探路棒，兴奋异常却不知道自己身在何处。</strong></p><p>所以以后我再也不会做错了，绝对不会：这就是我丢人现眼的失败主义，这份心境在大雪纷飞的黑夜中伴随着我咬牙切齿、凌冽前行。</p><p>愿心怀理想的诸位心拥白槿，满怀馨香。</p><blockquote><p>松树千年终是朽，槿花一日自成荣。——白居易</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>雑談</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japan</tag>
      
      <tag>Education</tag>
      
      <tag>China</tag>
      
      <tag>Politics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>中文翻译: 详细的PCA解析</title>
    <link href="/2020/05/05/pcaExplain/"/>
    <url>/2020/05/05/pcaExplain/</url>
    
    <content type="html"><![CDATA[<blockquote><p>原文没有写版权声明，所以我对原作者进行了署名，并且并非进行商业活动。<br><br/>不过鉴于原文没有注明不得增加额外限制，所以读者使用本翻译文本需要遵守署名-非商业性使用-相同方式共享 BY-NC-SA的协议。<br/><br>原文的作者: Nov 16, 2019 • Casey Juanxi Li<br/><br>翻译: Nov 19, 2019 • VCVina<br/><br>原文链接<a href="https://notsquirrel.com/pca/" target="_blank" rel="noopener">https://notsquirrel.com/pca/</a></p></blockquote><p>PCA是我在学数据分析的时候一头雾水的门类，不过那是去年的事了。</p><p>今天我在复习PCA的时候，偶然找到自己喜欢的科普博主@3Blue1Brown 转发了一个博客，就是有关这个的，我发现写的蛮好的，于是一边学一边翻译，可能有疏漏，以后会改的。</p><h1 id="图像的协方差矩阵"><a href="#图像的协方差矩阵" class="headerlink" title="图像的协方差矩阵"></a>图像的协方差矩阵</h1><p>什么是图像的协方差矩阵？让我们开始有关这个问题的探讨。<br>假设我们有一个X，这其实是一个图像，只不过被从D × D压缩为了1 × D²。这个过程就如下文的GIF演示一般。</p><p><img src="/img/postsImg/pcaExplain/cov1.gif" srcset="/img/loading.gif" alt="gif"></p><p>为了让问题更加简单，我们假设图像没有颜色通道（比如RGB之类的），这是一个纯灰度的图片，每个像素点带着从0到255的灰度值。我们接下来的工作就是进行「归一化」（normalize），将每个像素的灰度值变成0到1的浮点数。</p><p><img src="/img/postsImg/pcaExplain/cov2.gif" srcset="/img/loading.gif" alt="gif"></p><p>所以一个图像X可以被表示成一个1 × D²维度的向量（接下来，为了方便描述，我们假设D为8，D²自然为 8乘9等于 64）</p><center>X=[0.0,0.7,0.65,…0.7,0.9]</center><p>很好。所以什么是协方差矩阵？</p><p>我们可以询问维基百科。对于一对像素集合Xi和Xj来说，这就意味着：</p><center>cov(Xi,Xj)=E[(Xi−E[Xi])(Xj−E[Xj])]</center><p>一般来说，我们可以把这个写成一个点乘式回避E(X)这个符号：</p><center>cov(X,X)=E[(X−μX)(X−μX)]</center><h1 id="等下…期望？μ？"><a href="#等下…期望？μ？" class="headerlink" title="等下…期望？μ？"></a>等下…期望？μ？</h1><p>我们刚才不是说了，我们在讨论的是一张图片里面的点阵吗？</p><p>确实，如果我们只有一张图片，那么Xi自然是等于E[Xi]，并且X也等于μx。计算单一图片的协方差是没有意义的，这种行为就像你给一个单一的样本计算平均数一样，所以不要着急，请继续看下去。</p><p>如果我们有更多的图像X，会发生什么？</p><p>假如我们有很多N个不同的样本Xi，比如说你现在有一堆手写的「2」的图像数据。</p><p>我们现在就有了一个N × 64的阵列（N个1 × 64），我们把这个集合命名为X，这看上去就像这样: </p><p><img src="/img/postsImg/pcaExplain/cov3.gif" srcset="/img/loading.gif" alt="gif"></p><p>这个X看上去就复杂的多了，为了方便我们之后的理解我们现在先梳理一下这个X即将用到的具体的属性和参数：</p><ul><li>存在一个索引值i，从1到64，可以索引到X其中一个图像中的所有像素点。<ul><li>我们会把i用于Xi</li></ul></li><li>存在另一个索引值n，从1到N，可以索引到X中任何一个图像。<ul><li>我们会把n用于 X(n)</li></ul></li><li>所以，X3(5)意味着「第五个图像的第三个像素点的灰度值」。</li></ul><p>计算μx：<br/><br>现在我们可以有意义的计算E[Xi]了，我们可以得到如下计算式: </p><p><img src="/img/postsImg/pcaExplain/compute.png" srcset="/img/loading.gif" alt="image"></p><p>数学符号什么的，那个是数学家的工作，我们只需要尽可能简单地了解这里究竟在干什么就好了！我们会如此描述：我们在计算所有图像中第i个像素位的灰度值的平均值，E[Xi]如下图所示一样简单明了：</p><p><img src="/img/postsImg/pcaExplain/cov4.gif" srcset="/img/loading.gif" alt="gif"></p><p>我们用这个算式从1加到64，我们就得到了64个不同的平均数（分别是所有图像的第一个像素平均值、所有图像的第二个像素平均值、所有图像的第三个像素平均值… …）</p><center>μX=[E[X1],E[X2],…E[X784]]</center><p><img src="/img/postsImg/pcaExplain/cov5.gif" srcset="/img/loading.gif" alt="gif"></p><h1 id="计算cov-X-X"><a href="#计算cov-X-X" class="headerlink" title="计算cov(X, X)"></a>计算cov(X, X)</h1><p>我们尝试去理解cov(X, X) = E[(X−μX)(X−μX)]。<br>我们先看看cov(Xi, Xj) = E[ (Xi−E[Xi]) (Xj−E[Xj]) ]：一个特定像素点组i（所有图第i个像素的组合）和像素点组j的协方差。</p><p>对于每个单独的图像来说，我们用这个方法计算出这个像素点组i的内容究竟有多么偏离（或者说，多么独立于）像素组j。</p><p>对于每一个我们计算中产生的i和j的对，我们都要将这两个数相乘。我们有多少个这样的对呢？因为它可以自己配上自己，所以有64×64个对。这些对可以如下图般整齐地排列成这样的正方形矩阵。</p><p><img src="/img/postsImg/pcaExplain/cov6.gif" srcset="/img/loading.gif" alt="gif"></p><p>别忘了，在我们的集合X中，有N个图像需要如这样一般进行处理，换句话说，我们会如此处理出N个方阵。<br>关于协方差还有几个性质需要补充（用通俗的话来形容，不一定是很严格的说法）：</p><ul><li>i变大，同时j也变大，说明两个组内的元素大体是同向变化，协方差就是正。</li><li>i变小，j变大，说明两个组内的元素反向变化，协方差是负的。</li><li>上述两点都是协方差的性质，不过同向和反向其实不是一个科学的概念，因此用这两个词主要重在理解那种暧昧的「趋势」意义。</li><li>而当这两种情况之间几乎没有什么联系时，cov是接近于0的。换言之，在这种情况下i和j没啥关系，你看到像素i的高值并不意味着就很好知道j的值了——因为协方差不会给你很多关于像素j的有用信息。</li></ul><h1 id="一个思想实验：在画布上做填充"><a href="#一个思想实验：在画布上做填充" class="headerlink" title="一个思想实验：在画布上做填充"></a>一个思想实验：在画布上做填充</h1><p>如果i是黑的，是否意味着j也是黑的、或者趋向于黑的？或者两者之间根本没有关系？我们设立了一些协方差 cov(Xi,Xj)，帮助我们衡量两个数字之间的「相关性」。这个协方差中的X实际上是很多很多别的样本中第i个元素所组成的集合，因此cov(Xi,Xj)的意思是：</p><p>所有样本中第i个元素的集合、和所有样本中第j个元素的集合之间的协方差。这听上去似乎很好，因为很显然这里面包含了一种数据预测的思想。</p><p>你可以通过别的样本那边进行统计，并用得到的协方差预测你自己这里的像素值。</p><p>当然我们也给你了协方差矩阵作为帮助：</p><center>first row of cov(X,X)=[cov(X1,X1),cov(X1,X2)…cov(X1,X64)]</center><p>我们还可以像这个一样，一路建立下去，直到建立完第64行，但是现在只有一行。</p><p>所以和我一起做一个思想实验吧：假设我给你了一个8×8的像素画布，和一个协方差矩阵（这个矩阵由你不知道的某个样本中的像素计算而成，与你眼前的这个画布用协方差关联）。你必须尝试填涂你眼前这个像素画布，也就是说，通过我给你的协方差矩阵，来猜测这个画布到底长什么样。</p><p>我先告诉你：你现在手上第五十个像素是纯黑的，所以你在第五十个像素中也填上了纯黑色，但是你能通过这个猜出剩下所有的像素吗？</p><p>hummm… …协方差矩阵cov(X,X)的第五十行就蕴含了第五十个像素和其他64个像素之间的关系，如果cov(X50，X64)是一个非常负的值，这实际上告诉了你第64号像素（在最右下角）实际上应该是非常白的，因为协方差非常负就表示该值和第五十号像素的颜色是几乎完全不同的。如果cov(X50，X19)是一个非常正值，那就说明19号像素应该是非常暗的（也就是说和，第五十号像素差不多一样，毕竟协方差就是这个意思）。</p><p>你应该发现了，因为我给了你一个第五十号像素的值，你就可以举一反六四，迅速将合理的猜测推广到整个图像上。</p><p>我们继续这个思想实验，我顺便又告诉你了第51号像素，并且又告诉你了52，53… …实际上你可以这样做64次，每次都可以调整你的对于整个图像的判断。</p><p>可是我这样子做的目的是什么？</p><p>假如我是用手写的「2」来给你提供协方差的，也就是说我给你的协方差矩阵是用很多个「2」得到的。虽然可能我用不同样本算出的协方差矩阵效果都不统一，但是你得到的东西应该看起来还是像一个「2」。</p><p>现在，如果我要你还原的图像和「2」真的没啥太大关系——比如说，只是图像顶部的一条黑线——会怎么样呢？通常情况下，我们不会期望这样一条黑线出现在「2」的图像中，毕竟他真的和「2」没啥关系，这鬼才能想到。所以它的像素协方差应该在0左右，也就是「完全没有关联」的意思，得到的只是一些噪声。</p><p>如果我们将协方差矩阵视为一种转换的方法，这能有什么作用？</p><p>回想上面的思想实验，我们可以观察到以下内容。请记住，我们的协方差矩阵是使用一堆「2」的不同图像计算的。</p><ul><li>将该协方差矩阵应用于2的图像，将趋于点亮与2关联的像素关系。</li><li>将协方差矩阵应用于看起来不太像2的图像只会产生噪点。</li></ul><p>因此，我们可以把这个协方差矩阵想像成一​​个天真的孩子，它只学会画2。如果我们给它看似与2一致的东西（例如，顶部曲线或底部直线），它将根据“这东西最好是「2」”的思维来填充图像的其余部分。如果我们给它一个与2不一致的东西，它可能只会增加一些毫无意义的噪音。</p><h1 id="那么协方差矩阵的特征向量-Eigenvectors-是什么？"><a href="#那么协方差矩阵的特征向量-Eigenvectors-是什么？" class="headerlink" title="那么协方差矩阵的特征向量(Eigenvectors) 是什么？"></a>那么协方差矩阵的<strong>特征向量(Eigenvectors)</strong> 是什么？</h1><p>我们先要知道一些和特征向量有关的常识：</p><ul><li>矩阵其实可以理解为是一个变换。</li><li>矩阵的特征向量能够帮助该矩阵进行不改变方向只改变幅度的变换。（这是线性代数-「特征值和特征向量」中学到的内容）</li><li>相应的特征值就是幅度的变化。</li><li>如果你还没有学过线性代数，现在就可以放弃了（或者说已经来不及了——毕竟前面已经涉及到了线代的内容，真是难为你了）。<br>“慢着……窝老人家不懂你所说的改变方向是什么意思？”</li></ul><p>实际上，人脑在展现三维以上物体的能力并不出色（其实根本没法做到），因此让我们考虑三像素图像的情况：</p><ul><li>想象一下值为[0，0.2，0.1]，[0，0.4，0.2]和[0，1.0，0.5]的9像素点阵灰度图。其实这些像素都指向同一个方向，只不过大小不一样而已（用初中数学来说，是向量长度）。</li></ul><p><img src="/img/postsImg/pcaExplain/ex1.png" srcset="/img/loading.gif" alt="image"></p><ul><li>现在想象一下值[0，1，1]，[1，1，0]，[1，0，1]的图。即使它们的大小相同，这些向量也不会指向相同的方向。</li></ul><p><img src="/img/postsImg/pcaExplain/ex2.png" srcset="/img/loading.gif" alt="image"></p><ul><li>特征向量的「不改变方向」的意义其实在于，表示这些像素可能被点得更亮或者变得更暗，但是他们之间的比例是保持不变的。这就是我们为什么使用特征向量作为新的基底的原因。</li></ul><p>现在应该更清楚一点：协方差矩阵的特征向量能够配合变换该矩阵，并且只会变亮或变暗图像 – 但是像素关系不会改变。</p><p>具体来说，使用具有大且正特征值的特征向量，其像素之间的对比在通过协方差矩阵变化后变得更加强烈。</p><p>我们可以想象，如果我们在一组特定的图像（即仅某个数字）上计算了协方差矩阵，则特征向量必然会突出这个图的代表型结构（至少在您的数据集中）。</p><p>实际上，我们应该能够将一定数量的这些特征向量一起使用，并得到非常像我们原始图像的东西。</p><p>换句话说，如果我们稍微「不像一点」，是不是可以降低计算量的同时保有我们所要的特征呢？</p><h1 id="使用特征值和特征向量进行「去相关」和「降维」：（Principal-Components-Analysis的本质）"><a href="#使用特征值和特征向量进行「去相关」和「降维」：（Principal-Components-Analysis的本质）" class="headerlink" title="使用特征值和特征向量进行「去相关」和「降维」：（Principal Components Analysis的本质）"></a>使用特征值和特征向量进行「去相关」和「降维」：（Principal Components Analysis的本质）</h1><p>如果我们试图通过累加一堆特征向量来构造图像，那么我们投入的每个特征向量都会添加其他特征向量中没有办法表达的信息。<br>例如：如果您在A点，而B点在您的西北方，那么仅向北走就不可能到达B点。您还必须向西走一些，这就是我们说的北和西相互正交。（因为单纯靠「北」无法走到西边的任何地方）</p><p>这就是PCA利用特征向量进行去相关的意义：通过使用特征向量作为表示图像的新基础，我们可以自动确保这个基底由正交向量组成，这个是线性代数中极为重要的内容。两个正交向量虽然不能互相表示，但是这恰恰表明了另外一个好处：</p><p>虽然两者不能互相表示，但是两者相互配合，恰好能够以最精简最方便处理的姿态表示出这个线性空间其余所有的向量。</p><p>那么降维呢？一种k×k协方差矩阵将会具有K个特征向量，但是你当然不需要全部使用。当从特定类别的数据计算特征向量时，您可以使用最少4个特征向量来获得MNIST数字的重构：</p><p><img src="/img/postsImg/pcaExplain/fea1.png" srcset="/img/loading.gif" alt="image"></p><p>译者注释: 其实降维的意思可以理解为抛弃其余特征向量，选择高效的其中一部分特征向量。因为在线性空间中，特征向量的个数其实就是意味着线性空间中元素所保有的维度，当特征向量减少了维度就会降低，这也是线性代数中，秩(Rank)为什么会和特征向量永远保持个数相等的几何原因。但是遗憾的是中华人民共和国境内很多线代教材都没有提到这一点，因此作为补充…</p><h1 id="如何找到特征向量？如何使用特征向量？-非重点"><a href="#如何找到特征向量？如何使用特征向量？-非重点" class="headerlink" title="如何找到特征向量？如何使用特征向量？(非重点)"></a>如何找到特征向量？如何使用特征向量？(非重点)</h1><p>奇异值分解（SVD）是指线性代数中对矩阵M运算并将其分解为以下组件的方式：</p><center>M = U Σ V</center><p>M 是我们试图为其寻找特征值的矩阵（在这种情况下，这是我们从所有图像中计算出的协方差矩阵）</p><p>U Σ V，中U的列是左奇异向量，V的列是右奇异向量。在协方差矩阵为半正定的情况下，U = V，其列是我们希望找到的特征向量。</p><p>一旦我们有了U，我们只需要从中获取所需要的特征向量即可，我们通常会选择特征值排名前n名的特征向量，然后我们将其应用于矩阵乘法，就成功完成了降维工作。</p><p>实际上，在Python中实现此操作非常简单-这是一个示例，说明了如何实现。</p><pre><code class="hljs Python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Pca</span>:</span>    <span class="hljs-string">'''</span><span class="hljs-string">    A simple class to run PCA and store the truncated U matrix for use on the test set.</span><span class="hljs-string">    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        self.W = <span class="hljs-literal">None</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pca_train</span><span class="hljs-params">(self, X, n=<span class="hljs-number">5</span>)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        Helper function to perform PCA on X and store the chosen eigenvectors from U.</span><span class="hljs-string"></span><span class="hljs-string">        If input X is T x d,</span><span class="hljs-string"></span><span class="hljs-string">        transformed X' is T x d'</span><span class="hljs-string"></span><span class="hljs-string">        Also saves the chosen columns of U to self.W.</span><span class="hljs-string">        </span><span class="hljs-string">        Based heavily on http://cs231n.github.io/neural-networks-2/.</span><span class="hljs-string">        '''</span>        <span class="hljs-comment"># PCA down to top n eigenvectors</span>        <span class="hljs-comment"># Assume input data matrix X of size [T x d]</span>        cov = np.dot(X.T, X) / X.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># get the data covariance matrix</span>        U,S,V = np.linalg.svd(cov)        <span class="hljs-comment"># eigenvectors are returned sorted by eigenvalue; get top n</span>        X_prime = np.dot(X, U[:,:n])                 <span class="hljs-comment"># save W = U[:,:n] for use again in testing; we only train</span>        <span class="hljs-comment"># PCA on the TRAIN SET to prevent data leakage!</span>        self.W = U[:,:n]        <span class="hljs-keyword">return</span> X_prime    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pca_transform</span><span class="hljs-params">(self, X)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        Use the previously trained U to transform an input X (i.e. test data)</span><span class="hljs-string">        '''</span>        <span class="hljs-keyword">assert</span> self.W <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, <span class="hljs-string">"pca_test() called when self.W is None. Did you call pca_train() first?"</span>        <span class="hljs-keyword">return</span> np.dot(X, self.W)</code></pre>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ZH</tag>
      
      <tag>Computer Science</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Memory Management</title>
    <link href="/2020/05/05/memoryManagement/"/>
    <url>/2020/05/05/memoryManagement/</url>
    
    <content type="html"><![CDATA[<h1 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h1><h2 id="What-is-“Input-Queue”"><a href="#What-is-“Input-Queue”" class="headerlink" title="What is “Input Queue” ?"></a>What is “Input Queue” ?</h2><p>Collection of processes on the disk that are waiting to be brought into memory to run the program.<br>When we want to put a process into our main memory, we use Input Queue and pick a process from it.<br><a id="more"></a></p><h2 id="The-multistep-processing-of-a-user-programming"><a href="#The-multistep-processing-of-a-user-programming" class="headerlink" title="The multistep processing of a user programming"></a>The multistep processing of a user programming</h2><p><img src="/img/postsImg/memoryManagement/sourceToMemory.png" srcset="/img/loading.gif" alt="image"><br>we use “Address” to detect those processes, but how? Is the address given by CPU same as the address in memory?</p><p>Absolutely, no. We have two kinds of address:</p><ol><li><strong>Logical address(Virtual address)</strong>: Is the address at which an item (memory cell, storage element, network host) appears to reside from the perspective of an executing application program.</li><li><strong>Physical address</strong>: Physical Address is a memory address that is represented in the form of a binary number on the address bus circuitry in order to enable the data bus to access a particular storage cell of main memory, or a register of memory mapped I/O device.</li></ol><h2 id="Overlay-“オーバーレイ”-and-Paging-“ページング”-in-virtual-memory-仮想記憶"><a href="#Overlay-“オーバーレイ”-and-Paging-“ページング”-in-virtual-memory-仮想記憶" class="headerlink" title="Overlay(“オーバーレイ”) and Paging(“ページング”) in virtual memory(仮想記憶)"></a>Overlay(“オーバーレイ”) and Paging(“ページング”) in virtual memory(仮想記憶)</h2><p>Target: Enable a process to be larger than the amount of memory allocated to it</p><ul><li>Keep in memory only those instructions and data that are needed at any given time.</li><li>Implemented by user, no special support needed from operating system, programming design of overlay structure is complex.</li></ul><p>What is Symbol Table(シンボルテーブル):</p><ul><li>Δ Symbol table is a data structure used by a language translator such as a compiler or interpreter, where each identifier (a.k.a. symbol) in a program’s source code is associated with information relating to its declaration or appearance in the source.</li></ul><h2 id="What-is-“Contiguous-Allocation”-“連続メモリ割り当て”"><a href="#What-is-“Contiguous-Allocation”-“連続メモリ割り当て”" class="headerlink" title="What is “Contiguous Allocation”(“連続メモリ割り当て”)?"></a>What is “Contiguous Allocation”(“連続メモリ割り当て”)?</h2><ul><li>Each process is contained in a single contiguous section of memory, this mechanism is called “Contiguous Allocation”(“連続メモリ割り当て”).</li><li>There are two partitions in the main memory:<ol><li>Resident operating system, usually held in low memory with interrupt vector(“割り込みベクタ”).</li><li>User processes then held in high memory.</li></ol></li></ul><blockquote><p>Memory is usually divided into two areas, one for the operating system and the other for user processes. The operating system can be located in low memory or high memory. The main factor affecting this decision is the location of the interrupt vector. Because the interrupt vector is usually located in low memory, programmers usually put the operating system page in low memory.<br><a href="https://wangchangchung.github.io/2017/06/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E8%BF%9E%E7%BB%AD%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/" target="_blank" rel="noopener">https://wangchangchung.github.io/2017/06/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E8%BF%9E%E7%BB%AD%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/</a></p></blockquote><p>However, because those allocation is contiguous, how can we protect one process from one another ?</p><h2 id="Memory-Protection"><a href="#Memory-Protection" class="headerlink" title="Memory Protection:"></a>Memory Protection:</h2><ul><li><strong>Relocation Register</strong>: which contains value of smallest physical address.</li><li><strong>Limit Register</strong>: which contains range of logical addresses.</li><li>Under the joint action of these two mechanisms, <strong>the MMU maps the logical address dynamically by adding the value in the relocation register</strong>, and each logical address must be less than the limit register (Limit Register can control the memory range of each process)</li></ul><p><img src="/img/postsImg/memoryManagement/register.png" srcset="/img/loading.gif" alt="image"></p><blockquote><p>重定位寄存器含有最小的物理地址值，界限地址寄存器含有逻辑地址的范围值(例如：重定位=100050， 界限=74500)。有了重定位寄存器和界限地址寄存器，每个逻辑地址必须小于界限地址寄存器。MMU动态的将逻辑地址加上重定位寄存器的值后映射成物理地址，映射后的物理地址再交送内存单元。<br><a href="https://wangchangchung.github.io/2017/06/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E8%BF%9E%E7%BB%AD%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/" target="_blank" rel="noopener">https://wangchangchung.github.io/2017/06/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E2%80%94%E8%BF%9E%E7%BB%AD%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/</a></p></blockquote><h2 id="Multiple-partition-Allocation"><a href="#Multiple-partition-Allocation" class="headerlink" title="Multiple-partition Allocation:"></a>Multiple-partition Allocation:</h2><ul><li><p>Hole: block of available memory;Holes of various size are scattered throughout memory.<br>When a process arrives, it is allocated memory from a hole large enough to accommodate it. OS maintains information about:</p><ol><li>allocated partitions</li><li>free partitions (hole)</li></ol></li><li><p>How to satisfy a request of size n from a list of free holes?</p><ol><li>First-fit: Allocate the first hole that is big enough.</li><li>Best-fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.</li><li>Worst-fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.</li></ol></li><li><p>Fragmentation (“フラグメンテーション”) :</p><ol><li>External Fragmentation(“外部断片化”): total memory space exists to satisfy a request, but it is not contiguous.</li><li>Internal Fragmentation(“内部断片化”): allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used.</li></ol></li></ul><p>Reduce external fragmentation by memory compaction:</p><blockquote><p>コンパクションとガベージコレクションの違い: <br/>メモリの断片化を解消する機能はコンパクション（memory compaction）と呼ばれ、実現方法によってはガベージコレクションと共にコンパクションも行う仕組みになっている。そのためコンパクションを含めてガベージコレクションと呼ぶ場合もあるが、厳密には区別される。<br><a href="https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%99%E3%83%BC%E3%82%B8%E3%82%B3%E3%83%AC%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3" target="_blank" rel="noopener">https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%99%E3%83%BC%E3%82%B8%E3%82%B3%E3%83%AC%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3</a></p></blockquote><p>Shuffle memory contents to place all free memory together in one large block. Compaction is possible only if relocation is dynamic (“動的再配置”) , and is done at execution time.</p><p>Compaction is one way to solve this problem, but there is another way is Paging, which is a classical method in modern OS.</p><h1 id="Paging"><a href="#Paging" class="headerlink" title="Paging"></a>Paging</h1><h2 id="Background-of-“Paging”"><a href="#Background-of-“Paging”" class="headerlink" title="Background of “Paging”"></a>Background of “Paging”</h2><ul><li>Divide <strong>physical memory (“物理アドレス”)</strong> into fixed-sized blocks called Frames(“フレイム”) (size is power of 2, between 512 bytes and 8192 bytes).</li><li>Divide <strong>logical memory (“仮想/論理アドレス”)</strong> into blocks of same size called Pages(“ページ”).</li></ul><p>Address generated by CPU is divided into:</p><ul><li><strong>Page number (p) (“ページ番号”)</strong>: used as an index into a page table which contains base address of each page in physical memory.</li><li><strong>Page offset (d) (“オフセット”)</strong>: combined with base address to define the physical memory address that is sent to the memory unit.</li></ul><p><img src="/img/postsImg/memoryManagement/lpa.png" srcset="/img/loading.gif" alt="image"></p><center><pre><code class="hljs compute">Page 0 &#x3D;&gt; page table (0,5) &#x3D;&gt; Frame 5Page 1 &#x3D;&gt; page table (1,6) &#x3D;&gt; Frame 6… …“a” &#x3D;&gt; (number, offset) &#x3D; (0, 0) &#x3D;&gt; page table (0,5) &#x3D;&gt; [Frame 5, +0] &#x3D; 4*5 + 0“f” &#x3D;&gt; (number, offset) &#x3D; (1, 1) &#x3D;&gt; page table (1,6) &#x3D;&gt; [Frame 6, +1] &#x3D; 4*5 + 1… …</code></pre></center><p>Paging is a form of <strong>dynamic relocation</strong>.<br>Every logical address is bound by the paging hardware to some physical address. Using paging is similar to using a table of base (or relocation) registers, one for each frame of memory.</p><h2 id="Use-“Paging”-to-solve-the-fragmentation-problem-External-Fragmentation"><a href="#Use-“Paging”-to-solve-the-fragmentation-problem-External-Fragmentation" class="headerlink" title="Use “Paging” to solve the fragmentation problem (External Fragmentation)"></a>Use “Paging” to solve the fragmentation problem (External Fragmentation)</h2><p>Using paging scheme, <strong>there is no external fragmentation, but may be some internal fragmentation</strong>, because each page-size is usually 4 bytes long, and process may be leave some space.</p><h2 id="Implementation-of-Page-Table"><a href="#Implementation-of-Page-Table" class="headerlink" title="Implementation of Page Table"></a>Implementation of Page Table</h2><p>Page table is kept in main memory.</p><ul><li>Page-table base register (PTBR) points to the page table.</li><li>Page-table length register (PTLR) indicates size of the page table.</li></ul><p><strong>What is Hierarchical Page Tables?</strong></p><p>Most modern computer systems support a large logical address space, in which the page table becomes excessively large.<br>So we would not want to allocate the page table contiguously in main memory, we break up the logical address space into multiple page tables.</p><p><img src="/img/postsImg/memoryManagement/htable.png" srcset="/img/loading.gif" alt="image"></p><h2 id="Feature-of-Paging"><a href="#Feature-of-Paging" class="headerlink" title="Feature of Paging"></a>Feature of Paging</h2><p>An important aspect of paging is the clear separation between the user’s view of memory and the actual physical memory:</p><p>The difference between the user’s view of memory and the actual physical memory is reconciled by the address-translation hardware.</p><h1 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h1><h2 id="What-is-Segmentation"><a href="#What-is-Segmentation" class="headerlink" title="What is Segmentation"></a>What is Segmentation</h2><p>Memory segmentation is a computer (primary) memory management technique of <strong>division of a computer’s primary memory into segments or sections</strong>.<br>In a computer system using segmentation, a reference to a memory location includes a value that identifies a segment and an offset (memory location) within that segment.</p><p><img src="/img/postsImg/memoryManagement/segmentation.png" srcset="/img/loading.gif" alt="image"></p><p>The difference between Page and Segmentation</p><ul><li>A page is of fixed block size, and segmentation is of variable size.</li><li>Page may lead to internal fragmentation, segmentation may lead to external fragmentation</li></ul><h2 id="Segmentation-table-different-from-page-table"><a href="#Segmentation-table-different-from-page-table" class="headerlink" title="Segmentation table (different from page table)"></a>Segmentation table (different from page table)</h2><p>Every segmentation has a Segment-table base register(STBR) and Segment-table length register(STLR).</p><h1 id="Virtual-Memory-仮想記憶"><a href="#Virtual-Memory-仮想記憶" class="headerlink" title="Virtual Memory(仮想記憶)"></a>Virtual Memory(仮想記憶)</h1><h2 id="What-is-virtual-memory"><a href="#What-is-virtual-memory" class="headerlink" title="What is virtual memory?"></a>What is virtual memory?</h2><p>Virtual memory is a technology of memory management in computer system. <strong>It makes the application think that it has continuous available memory</strong> (a continuous and complete address space), but in fact, <strong>it is usually divided into multiple physical memory fragments, and some of them are temporarily stored on the external disk storage</strong> for data exchange when necessary.<br>Compared with the system without virtual memory technology, the system using this technology makes the <strong>writing of large programs easier and the use of real physical memory (such as RAM) more efficient.</strong></p><h2 id="Demand-Paging-デマンドページング-Bring-a-page-into-memory-only-when-it-is-need"><a href="#Demand-Paging-デマンドページング-Bring-a-page-into-memory-only-when-it-is-need" class="headerlink" title="Demand Paging(デマンドページング): Bring a page into memory only when it is need."></a>Demand Paging(デマンドページング): Bring a page into memory only when it is need.</h2><p><img src="/img/postsImg/memoryManagement/demandPage.png" srcset="/img/loading.gif" alt="image"></p><p>When there is no free frame: Page Replacement(ページ置換):</p><ul><li>What’s that mean?<br>Find some page in memory, but not really in use, swap it out.</li><li>Prevent Page Fault(“<a href="https://ja.wikipedia.org/wiki/%E3%83%9A%E3%83%BC%E3%82%B8%E3%83%95%E3%82%A9%E3%83%BC%E3%83%AB%E3%83%88" target="_blank" rel="noopener">ページフォールト</a>”):<br/><br>Prevent over-allocation of memory by modifying page-fault service routine to include page replacement.<br>Page replacement completes separation between logical memory and physical memory – large virtual memory can be provided on a smaller physical memory.</li><li>Of course, we want lowest page fault rate.</li><li>Paging Replacement Algorithm: FIFO, Least Recently Used, Counting (including LFU, MFU)</li></ul><p>Benefits of virtual memory:</p><ul><li>Copy-on-write:<ol><li>Allows both parent and child process to initially share the same pages in memory.</li><li>If either process modifies a shared page, it will modify a new page copied from the shared page.</li></ol></li><li>memory -mapped file:<ol><li>Allows file I/O to be treated as routine memory access by mapping a disk block to a page in memory.</li><li>A file is initially read using demand paging. A page-sized portion of the file is read from the file system into a physical page. Subsequent reads/writes to/from the file are treated as ordinary memory accesses.</li></ol></li></ul><h2 id="Allocate-the-frame"><a href="#Allocate-the-frame" class="headerlink" title="Allocate the frame"></a>Allocate the frame</h2><p>Schemes:</p><ul><li>Fixed allocation:<br>If 100 frames and 5 processes, give each process 20 pages.</li><li>Priority allocation:<br>According to the size of process. select for replacement a frame from a process with lower priority number.</li><li>Global allocation:<br>One process can take a frame from another.<br>If another process has a old frame, you can replace that frame, but local allocate could only check it’s own frames’ status.</li></ul><h2 id="Thrashing"><a href="#Thrashing" class="headerlink" title="Thrashing:"></a>Thrashing:</h2><p>A process is busy swapping pages in and out. It will lead to:</p><ul><li>Low CPU utilization.</li><li>OS thinks that it needs to increase the degree of multiprogramming. another process added to the system.</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Computer Science</tag>
      
      <tag>JA</tag>
      
      <tag>EN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>中日英专业术语对照</title>
    <link href="/2020/05/04/chinese-english/"/>
    <url>/2020/05/04/chinese-english/</url>
    
    <content type="html"><![CDATA[<p>简单的词汇对照表，在做外语的题目时可能会有帮助。<br><a id="more"></a></p><h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><p>偶(ぐう)奇(き)性(せい)——奇偶性</p><p>はさみうちの原理——夹逼定理</p><p>微(び)分(ぶん)——微分</p><p>全微分——全微分</p><p>偏(へん)微分——偏微分</p><p>積(せき)分(ぶん)——积分</p><p>不(ふ)定(てい)積分——不定积分</p><p>微(び)分(ぶん)可(か)能(のう)——可微</p><p>函(かん)数(すう)・関(かん)数(すう)——函数</p><p>陰(いん)函数——隐函数implicit function</p><p>逆(ぎゃく)関数——逆函数</p><p>変(へん)数——变量</p><p>導関数——导函数</p><p>二階導関数——二阶导函数</p><p>微分係(けい)数(すう)——斜率</p><p>媒(ばい)介(かい)変数——参数</p><p>定数——常数</p><p>独(どく)立(りつ)変数——自变量</p><p>従(じゅう)属(ぞく)変数——应变量</p><p>平(へい)均(きん)値(ち)の定(てい)理(り)——中值定理</p><p>ラグランジュ——拉格朗日</p><p>コーシー ——柯西</p><p>ロピタル——洛必达</p><p>有(ゆう)限(げん)増(ぞう)分(ぶん)の定(てい)理(り)——有限增量定理</p><p>不(ふ)定(てい)形(けい)——未定式</p><p>テイラー(テーラー)の定理——泰勒定理</p><p>テイラー(テーラー)多(た)項(こう)式(しき)——泰勒多项式</p><p>テイラー(テーラー)展(てん)開(かい)——泰勒展开</p><p>ペアノの剰(じょう)余(よ)項(こう)——皮亚诺余项</p><p>マクローリン——麦克劳林</p><p>第(だい)一(い)種(っしゅ)不(ふ)連(れん)続(ぞく)点——第一类间断点</p><p>除(じょ)去(きょ)可能な不連続点——可去间断点</p><p>跳(ちょう)躍(やく)不連続点——跳跃间断点</p><p>単(たん)調(ちょう)性(せい)——单调性</p><p>単調増(ぞう)加(か)(減(げん)少(しょう))関数——单调递增(递减)函数</p><p>定(てい)義(ぎ)域(いき)/始(し)域(いき)——定义域</p><p>部(ぶ)分(ぶん)積(せき)分(ぶん)——分部积分法</p><p>接(せっ)線(せん)——切线</p><p>勾(こう)配(ばい)——梯度gradient</p><p>方(ほう)向(こう)微分——方向导数directional derivative</p><p>クロネッカーのデルタ——克罗内克函数</p><p>スカラー(ドット)積——点乘积</p><p>発散——散度divergence</p><h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><p>ベクトル——vector向量</p><p>ノルム——范数</p><p>複(ふく)素(そ)数(すう)——复数</p><p>線(せん)型(けい)写(しゃ)像(ぞう)——线性映射(变换矩阵就是一种)</p><p>線型独立——线性无关</p><p>線型従(じゅう)属(ぞく)——线性相关</p><p>線型(ベクトル)空間——线性(向量)空间</p><p>線型結(けつ)合(ごう)——线性组合</p><p>固(こ)有(ゆう)値(ち)——特征值eigenvalues</p><p>固有空(くう)間(かん)——特征空间eigenvectors</p><p>ヒルベルト空間——希尔伯特空间</p><p>ユークリッド空間——欧几里得空间</p><p>内(ない)積(せき)——内积</p><p>(非(ひ))斉(せい)次(じ)方(ほう)程(てい)式(しき)——齐次方程</p><p>行(ぎょう)列(れつ)——矩阵（日本的行row列column定义和我们一致）</p><p>区分け——分块</p><p>交代行列——反对称矩阵</p><p>ユニタリ行列——酉矩阵</p><p>Aの行列式(しき)——A的行列式「det(A)」</p><p>転(てん)置(ち)——转置</p><p>冪(べき)乗(じょう)——幂乘</p><p>随(ずい)伴(はん)行列——伴随矩阵「Adj(A)，A*」</p><p>余(よ)因(いん)子(し)——代数余子式cofactor</p><p>小(しょう)行列式——余子式minor</p><p>正(せい)則(そく)(非(ひ)特(とく)異(い))行列——非奇异矩阵（可逆不等于0）</p><p>行列の階数 / ランク——秩</p><p>直(ちょ)交(っこう)——正交</p><p>相(そう)似(じ)——相似</p><p>自明な線型関係——全零解体现的线性关系</p><p>基(き)底(てい)——基向量basis</p><p>実対称行列——实对称矩阵real symmetric matrix</p><p>二次形式——二次型</p><p>合同行列——合同矩阵</p><p>同(どう)値(ち)関係——等价关系</p><p>正/負の慣性指数——正负惯性指数</p><p>シルヴェスターの慣性法­則——西尔维斯特惯性定理­</p><p>正(せい)定(てい)値(ち)——正定</p><h1 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h1><p>配列——数组</p><p>スタック——栈stack</p><p>最小堆min heap</p><p>木(き)構(こう)造(ぞう)——树(型数据结构)</p><p>連結リスト——链表</p><p>2分探索木——二叉搜索树</p><p>深(ふか)さ優(ゆう)先(せん)探(たん)索(さく)/縦型探索——深度优先遍历</p><p>幅(はば)優先探索/横型探索——广度优先遍历</p><p>前順・先(せん)行(こう)順・前置順・行きがけ順——先根序</p><p>間順・中間順・通りがけ順——中根序</p><p>後順・後行順・後置順・帰りがけ順——后根序</p><p>構造体——结构体</p><p>抽(ちゅう)象(しょう)データ型(がた)——抽象数据类型</p><p>グラフ理論——图论</p><p>ハッシュ表——哈希表</p><p>動的計画法——动态规划法dymastic</p><p>貪(どん)欲(よく)法(ほう)——贪心算法Greed method</p><p>分(ぶん)割(かつ)統(とう)治(ち)法(ほう)——分治算法divide-and-conquer method</p><p>列(れっ)挙(きょ)型(がた)——枚举</p><p>挿入ソート——插入排序</p><p>シェルソート——希尔排序Shell sort</p><p>バブルソート——冒泡排序</p><p>クイックソート——快速排序</p><p>線形リスト——线性表</p><p>文字列——字符串string</p><p>根ノード——根节点</p><p>葉ノード——叶节点</p><p>高さ——高度</p><p>深さ——深度</p><p>部分木——子树</p><p>子/親/兄弟/子孫/内部/先祖ノード——子/兄弟/子孙/内部/先祖节点</p><p>完全二分木——完全二叉树</p><p>(逆)ポーランド記法——(逆)波兰表达式（前/后缀表达式）</p><p>ヒープ——堆</p><p>ハフマン符号(木)——哈夫曼编码/树</p><p>データ圧(あっ)縮(しゅく)——数据压缩</p><p>ベン図——韦恩图Venn diagram</p><p>完全/有向/無向/連(れん)結(けつ)グラフ——完全/有向/无向/连通图complete graph</p><p>入/出次数——入/出度in/outdegree</p><p>サブ/部分グラフ——子图subgraph</p><p>重み付きグラフ——带权图weighted graph</p><p>長さ——长度(边数)</p><p>距離——距离(最短边数)</p><p>直(ちょっ)径(けい)——直径(最长边数)</p><p>道/パス——路径path</p><p>単純閉道——简单回路</p><p>連結成分——最大连通子图(连通分量)</p><p>隣(りん)接(せつ)行列——相邻(邻接)矩阵</p><p>(最小)全(ぜん)域(いき)木(ぎ)——(最小)生成树(minimum)spanning tree</p><p>バケットソート——桶排序bucket sort</p><p>操車場アルゴリズム——调度场算法Shunting-yard algorithm(用于逆波兰表达式)</p><p>例(れい)外(がい)処(しょ)理(り)——差错处理</p><h1 id="分析情報学基礎第1分野"><a href="#分析情報学基礎第1分野" class="headerlink" title="分析情報学基礎第1分野"></a>分析情報学基礎第1分野</h1><h2 id="数理统计"><a href="#数理统计" class="headerlink" title="数理统计"></a>数理统计</h2><p>ランダム——随机的(Random)</p><p>ランダム性(せい)/無(む)作(さく)為(い)性(せい)——随机性</p><p>組合せの数——组合数</p><p>ベイズ推(すい)定(てい)——贝叶斯推断</p><p>（仮(か)説(せつ)）検定——假设检验</p><p>全確率の公式——全概率公式(Law of Total Probability)</p><p>(累(るい)積(せき))分(ぶん)布(ぷ)関数——分布函数</p><p>(標準)正(せい)規(き)分(ぶん)布(ぷ)——标准正态分布normal distribution</p><p>ガウス分布——高斯分布(正态分布另一个名字)</p><p>相(そう)関(かん)係(けい)数(すう)——相关系数</p><p>決(けっ)定(てい)係数——决定系数</p><p>全平方和——总平方和SST</p><p>残(ざん)差(さ)平(へい)方(ほう)和(わ)——残差平方和SSE</p><p>回(かい)帰(き)平方和——回归平方和SSR</p><p>説明変数——解释变量(自变量)</p><p>ピアソン積(せき)率(りつ)相関係数——皮尔逊积矩相关系数</p><p>平(へい)均(きん)（期待）——数学期望</p><p>分(ぶん)散(さん)——方差</p><p>標準偏差——标准差</p><p>共(きょう)分(ぶん)散(さん)——协方差</p><p>ポアッソン分布——泊松分布</p><p>一様分布（矩形分布）——均匀分布</p><p>正(せい)規(き)分布——正态分布</p><p>大数の法則——大数定理</p><p>中心極(きょく)限(げん)定(てい)理(り)——中心极限定理</p><p>尤(ゆう)度(ど)関(かん)数(すう)——似然函数</p><p>最尤推定——最大似然估计</p><p>交(こう)差(さ)検(けん)証(しょう)——交叉验证</p><p>同時/結(けつ)合(ごう)確率分(ぶん)布(ぷ)——联合概率分布</p><p>条件付(つ)き確率——条件概率</p><p>周(しゅう)辺(へん)分布——边际分布</p><p>有意水準——显著性水平α</p><p>有意差——显著性差异</p><p>帰(き)無(む)仮説——原假设</p><p>対立仮説——对立假设</p><h2 id="データ分析（分析情報学基礎1分野）"><a href="#データ分析（分析情報学基礎1分野）" class="headerlink" title="データ分析（分析情報学基礎1分野）"></a>データ分析（分析情報学基礎1分野）</h2><p>多変量解析——多元变量统计（多变量降维的那个方法）</p><h2 id="機械学-がく-習-しゅう-（分析情報学基礎1分野）"><a href="#機械学-がく-習-しゅう-（分析情報学基礎1分野）" class="headerlink" title="機械学(がく)習(しゅう)（分析情報学基礎1分野）"></a>機械学(がく)習(しゅう)（分析情報学基礎1分野）</h2><p>ディープラーニング——Deep Learning</p><p>カーネル(kernel)法——核化法</p><p>サポートベクターマシン——支持向量机（SVM）</p><p>ランダムフォレスト——随机森林</p><p>ニューラル（neural）ネットワーク——人工神经网络</p><p>損(そん)失(しつ)関数——损失函数(loss function)</p><p>過(か)剰(じょう)適(てき)合(ごう)/過適合/過学(がく)習(しゅう)——过拟合</p><p>最小二(じ)乗(じょう)法——最小二乘法</p><p>経験損(そん)失(しつ)——经验风险(Empirical Risk)</p><p>教師あり学習——监督学习</p><p>教師なし学習——非监督学习</p><p>半教師有り学習——半监督学习</p><p>最(さい)急(きゅう)降(こう)下(か)法(ほう)——梯度下降法Gradient descent</p><p>価値関数——值函数</p><p>方策——办法(很空泛的词汇，比如Q学习就是强化学习的一种方策)</p><p>単純ベイズ分類器 (朴素贝叶斯分类器)——朴素贝叶斯分类器</p><p>自己組織化写像——SOM自组织映射Self-organizing maps</p><p>敵対的生成ネットワーク——GAN生成式对抗网络Generative adversarial networks</p><p>制限ボルツマン・マシン——RBM受限玻尔兹曼机Restricted Boltzmann machine</p><p>価値関数——价值函数</p><p>クラスタリング——聚类clustering</p><p>分裂Split</p><h1 id="コンピュータ-ソフトウェア（计算机科学软件部分）"><a href="#コンピュータ-ソフトウェア（计算机科学软件部分）" class="headerlink" title="コンピュータ　ソフトウェア（计算机科学软件部分）"></a>コンピュータ　ソフトウェア（计算机科学软件部分）</h1><h2 id="オペレーティングシステム-操作系统"><a href="#オペレーティングシステム-操作系统" class="headerlink" title="オペレーティングシステム (操作系统)"></a>オペレーティングシステム (操作系统)</h2><p>並行性——并发</p><p>食事する哲学者の問題——哲学家吃饭问题Dining Philosophers Problem</p><p>スピンロック——自旋锁Spinlock</p><p>仮(か)想(そう)記(き)憶(おく)——虚拟内存</p><p>主記憶装置/メインメモリ——主存main memory</p><p>対称型マルチプロセッシング——对称多处理</p><p>疎(そ)/密(みつ)結(けつ)合(ごう)——松/紧耦合</p><p>Random access memory——RAM随机存取存储器</p><p>割り込み——中断interrupt</p><p>レジスター——寄存器register</p><p>アセンブリ言語——汇编语言assembly language</p><p>ランタイムライブラリ——运行时库run-time library</p><p>プロセス制御ブロック——进程控制快Process control block</p><p>プロセス間通信——进程间通信InterProcess Communication(IPC)</p><p>同期——同步Synchronization</p><p>パイプ——管道pipe(IPC方式)</p><p>通信端点/ソケット——套接字socket</p><p>ユーザー/カーネルスレッド——用户/核进程</p><p>ホモジニアス/ヘテロジニアス・マルチコア——同构/异构多核Homogeneous/Heterogeneous multicore</p><p>競(きょう)合(ごう)状態/レースコンディション——竞争条件Race Condition</p><p>クリティカルセクション/危険領(りょう)域(いき)——临界区段Critical Section</p><p>排(はい)他(た)制(せい)御(ぎょ)/相互排他——互斥锁(方法)Mutual Exclusion</p><p>モニタ——管程Monitors</p><p>オーバーレイ——覆盖overlay</p><p>シンボルテーブル——符号表symbol table</p><p>セグメント方式——内存分段segment</p><h2 id="ネットワーク-计算机网络软件视角"><a href="#ネットワーク-计算机网络软件视角" class="headerlink" title="ネットワーク(计算机网络软件视角)"></a>ネットワーク(计算机网络软件视角)</h2><p>Simplex communication——单工通讯</p><p>経(けい)路(ろ)制(せい)御(ぎょ)——路由理论routing</p><p>通信プロトコル——网络传输协议</p><p>暗号(ごう)化——加密</p><p>輻(ふく)輳(そう)制(せい)御(ぎょ)——阻塞控制</p><p>符(ふ)号(ごう)化(か)方(ほう)式(しき)——编码方式</p><p>マンチェスタ符号——曼彻斯特编码</p><p>単(複)流非ゼロ復帰——不归零编码NRZ</p><p>標(ひょう)本(ほん)化(か)定理——奈奎斯特–香农采样定理</p><p>伝(でん)送(そう)路——信道channel</p><p>単(双)方向——单(双)工Simplex/Duplex</p><p>帯(たい)域(いき)幅(はば)/帯域/バンド幅——带宽Bandwidth</p><p>搬(はん)送(そう)波(は)——载波carrier wave</p><p>SN比/信号雑音比——信噪比signal-noise ratio</p><p>シャノンの通信路符号化定理——香农信号编码定理</p><p>多(た)重(じゅう)化(か)——多路复用multiplexing</p><p>周(しゅう)波(は)数(すう)——频率Frequency</p><p>変調——调制modulation</p><p>復調——解调</p><p>符号/時/周波数/空間 分割多重化——码/时/频/空分复用</p><p>制御文字——控制字符control character(例如ESC这种带转义功能的字符)</p><p>エスケープ文字——转义字符</p><p>巡(じゅん)回(かい)冗(じょう)長(ちょう)検(けん)査(さ)——循环冗余检查CRC</p><p>肯定応答/ACK——确认字元Acknowledge character</p><p>自動再送要求——ARQ自动重传Auto Repeat reQuest</p><p>Stop-and-wait / Go-Back-N / Selective Repeat ARQ——停等/连续/选择重传ARQ</p><p>ビットスタッフィング——比特位填充bit stuffing</p><p>ゼロビット挿入——零比特填充</p><p>情報フレーム——信息帧(HDLC中的术语)</p><p>制御専用フレーム——监督帧</p><p>非番号制フレーム——无编码帧</p><p>イーサネット——以太网Ethernet</p><p>搬送波感知多重アクセス/衝突検出——CSMA/CD载波侦听多路访问/碰撞检测Carrier Sense Multiple Access</p><p>トークンバス——令牌总线</p><p>トークンリング——令牌环</p><p>マルチキャスト——多播multicast</p><p>エニーキャスト——任波anycast</p><p>ブロードキャスト——广播broadcast</p><p>ユニキャスト——单播unicast</p><p>ベースバンド——基带baseband</p><p>バンドパスフィルタ——带通</p><p>ネットワークカード——网卡Network Interface Card</p><p>同(どう)軸(じく)ケーブル——同轴电缆</p><p>ツイストペアケーブル——双绞线twisted pairs cable</p><p>光ファイバー——光纤</p><p>競合期間——争用期contention period time</p><p>仮想回線——虚电路服务virtual circuit (网络层)</p><p>コネクションレス型通信(データグラム)——数据报服务datagram</p><p>非同期転送/アシンクロナス トランスファー モード——异步传输模式(ATM)Asynchronous Transfer Mode</p><p>パケット通信——分组交换packet switching</p><p>経路表——路由表routing table</p><p>サブネットマスク——子网掩码subnet mask</p><p>ネットワークアドレス変換——网络地址转换Network Address Translation</p><p>可変長サブネットマスク——可变长子网掩码VLSM</p><p>サイダー/CIDR——无类别域间路由</p><p>サブネット——子网subnet</p><p>ARPスプーフィング——ARP欺骗ARP spoof</p><p>Maximum Transmission Unit——最大传输单元</p><p>リンクステート型ルーティングプロトコル——链路状态路由协议Link State Routing protocol</p><p>距離ベクトル型ルーティングプロトコル——距离向量路由协议Distance-vector routing protocol</p><p>インターネット割当番号公社——互联网号码分配局IANA</p><p>ネットワークエッジ——网络边缘(端系统)</p><p>トップレベルドメイン(TLD)——顶级域名Top-level Domain</p><p>簡易メール転送プロトコル——SMTP简易邮件传输协议</p><p>郵局協定——邮局协议Post Office Protocol</p><p>(D)DoS攻撃/(分散型)サービス妨害攻撃  ——(分布式)拒绝服务攻击(Distributed) Denial of Service attack</p><p>踏み台——肉鸡</p><p>共(きょう)通(つう)鍵(かぎ)暗(あん)号(ごう)——对称加密common key cryptosystem</p><p>公開鍵暗号——非对称加密（公钥加密）</p><h2 id="プログラミング言-げん-語-ご-计算机编程语言"><a href="#プログラミング言-げん-語-ご-计算机编程语言" class="headerlink" title="プログラミング言(げん)語(ご)(计算机编程语言)"></a>プログラミング言(げん)語(ご)(计算机编程语言)</h2><p>反(はん)復(ぷく)(繰り返し)——迭代iteration</p><p>再(さい)帰(き)——递归recursion</p><p>末(まつ)尾(び)再(さい)帰(き)——尾递归</p><p>演(えん)算(ざん)子(し)定(てい)義(ぎ)——操作符重载</p><h1 id="コンピュータ-ハードウェア（计算机科学硬件部分）"><a href="#コンピュータ-ハードウェア（计算机科学硬件部分）" class="headerlink" title="コンピュータ　ハードウェア（计算机科学硬件部分）"></a>コンピュータ　ハードウェア（计算机科学硬件部分）</h1><h2 id="論理回路-数字逻辑电路"><a href="#論理回路-数字逻辑电路" class="headerlink" title="論理回路(数字逻辑电路)"></a>論理回路(数字逻辑电路)</h2><p>スイッチング理論——数理逻辑(离散数学)</p><p>論理代(だい)数(すう)——逻辑代数(布尔代数)</p><p>組み合わせ論理回路——组合逻辑电路</p><h2 id="コンピュータ-アーキテクチャ-计算机组成原理"><a href="#コンピュータ-アーキテクチャ-计算机组成原理" class="headerlink" title="コンピュータ アーキテクチャ(计算机组成原理)"></a>コンピュータ アーキテクチャ(计算机组成原理)</h2><p>仮想化——虚拟化</p><p>マイクロプログラム——微程式</p><h2 id="コンピュータネットワーク-计算机网络硬件视角"><a href="#コンピュータネットワーク-计算机网络硬件视角" class="headerlink" title="コンピュータネットワーク(计算机网络硬件视角)"></a>コンピュータネットワーク(计算机网络硬件视角)</h2><p>ネットワークプロセッサ——网络处理器</p><p>インタフェース——接口(硬件，例如USB)</p><p>伝(でん)送(そう)路——信道</p><h1 id="微观经济学"><a href="#微观经济学" class="headerlink" title="微观经济学"></a>微观经济学</h1><p>ミクロ経済学ーーmicroeconomics, 微观经济学</p><p>資源配分——resource allocation</p><p>事実解明的ーーpositive, 实证</p><p>規範的ーーnormative, 规范</p><p>選好ーーpreference</p><p>効用関数ーーutility function</p><p>顕示選好の理論ーーrevealed preference theory</p><h1 id="宏观经济学"><a href="#宏观经济学" class="headerlink" title="宏观经济学"></a>宏观经济学</h1><p>マクロ経済学ーーmacroeconomics, 宏观经济学</p><p>国民経済計算ーーSystem of National Accounts, SNA</p><p>フロー変数ーーflow, 流量</p><p>ストック変数ーーstock, 存量</p><p>キャピタル・ゲーン / キャピタル・ロスーーcapital gain / capital loss</p><h1 id="做题用的术语："><a href="#做题用的术语：" class="headerlink" title="做题用的术语："></a>做题用的术语：</h1><p>merits and demerits/pros and cons——利与弊</p><p>セッション——session会话</p><p>ペイロード——有效承载信息量payload</p><p>layout——包的组成结构</p><p>データ・オフセット——data offset在TCP中是头部长度的意思。</p><p>execution time——执行时间</p><p>autonomous system——自治系统</p><p>…による——…所带来的</p><p>shared variable——共享变量</p><p>time interval——时间间隔</p><p>high load——高负载</p><p>a mixture of the two methods——两种方法的混合</p><p>orthogonal——正交</p><p>dot product——点积</p><p>predict——预测</p><p>Rotational Latency——转动周期</p><p>functionality——功能</p><p>encapsulate——封装</p><p>Repeat the previous step——重复上述步骤</p><p>handover——交接</p><p>access permission——访问权限</p><p>Bellman formula——贝尔曼方程</p><p>syntax——句法</p><p>certain functionality——特定功能</p><p>threshold——阈值</p><p>frequency spectrum——频谱<br><!--stackedit_data:eyJoaXN0b3J5IjpbNzU4MzM4MTc0LDE2MzQ3ODM2MTVdfQ==--></p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ZH</tag>
      
      <tag>JA</tag>
      
      <tag>EN</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
